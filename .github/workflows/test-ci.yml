# ABOUTME: æ¸¬è©¦ CI workflows æœ‰æ•ˆæ€§çš„ç°¡åŒ–ç‰ˆæœ¬
# ABOUTME: ç”¨æ–¼å¿«é€Ÿé©—è­‰ CI é…ç½®æ˜¯å¦æ­£ç¢ºï¼Œä¸åŸ·è¡Œå®Œæ•´çš„æ¸¬è©¦å¥—ä»¶

name: Test CI Configuration

on:
  workflow_dispatch:
    inputs:
      test-component:
        description: 'Component to test'
        required: false
        default: 'basic'
        type: choice
        options:
          - 'basic'
          - 'security'
          - 'benchmark'
          - 'matrix'

jobs:
  test-basic:
    name: æ¸¬è©¦åŸºæœ¬é…ç½®
    runs-on: ubuntu-latest
    if: github.event.inputs.test-component == 'basic' || github.event.inputs.test-component == ''
    
    steps:
    - name: ðŸ“¥ Checkout repository
      uses: actions/checkout@v4
    
    - name: ðŸ”§ Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
    
    - name: ðŸ Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"
    
    - name: ðŸ”— Create virtual environment
      run: uv venv
    
    - name: ðŸ“¦ Install dependencies
      run: |
        source .venv/bin/activate
        uv sync --dev
    
    - name: ðŸ§ª Run basic tests
      run: |
        source .venv/bin/activate
        uv run pytest src/core/tests/unit/config/test_base_settings.py::TestBaseCoreSettings::test_default_values -v
    
    - name: âœ… Basic configuration test passed
      run: echo "âœ… Basic CI configuration is working correctly"

  test-security:
    name: æ¸¬è©¦å®‰å…¨æŽƒæé…ç½®
    runs-on: ubuntu-latest
    if: github.event.inputs.test-component == 'security'
    
    steps:
    - name: ðŸ“¥ Checkout repository
      uses: actions/checkout@v4
    
    - name: ðŸ”§ Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
    
    - name: ðŸ Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"
    
    - name: ðŸ”— Create virtual environment
      run: uv venv
    
    - name: ðŸ“¦ Install dependencies
      run: |
        source .venv/bin/activate
        uv sync --dev
    
    - name: ðŸ” Test security tools
      run: |
        source .venv/bin/activate
        echo "Testing pip-audit..."
        uv run pip-audit --format=json > test-audit.json
        echo "âœ… pip-audit works"
        
        echo "Testing bandit..."
        uv run bandit -r src/core/core/config/ -f json > test-bandit.json || echo "âš ï¸ bandit found issues (normal)"
        echo "âœ… bandit works"
        
        echo "Testing safety (may fail due to known issues)..."
        uv run safety --version || echo "âš ï¸ safety has issues"
    
    - name: ðŸ“„ Upload test results
      uses: actions/upload-artifact@v4
      with:
        name: test-security-results
        path: |
          test-audit.json
          test-bandit.json
        retention-days: 7
    
    - name: âœ… Security configuration test passed
      run: echo "âœ… Security scanning configuration is working"

  test-benchmark:
    name: æ¸¬è©¦ Benchmark é…ç½®
    runs-on: ubuntu-latest
    if: github.event.inputs.test-component == 'benchmark'
    
    steps:
    - name: ðŸ“¥ Checkout repository
      uses: actions/checkout@v4
    
    - name: ðŸ”§ Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
    
    - name: ðŸ Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"
    
    - name: ðŸ”— Create virtual environment
      run: uv venv
    
    - name: ðŸ“¦ Install dependencies
      run: |
        source .venv/bin/activate
        uv sync --dev
    
    - name: ðŸ“ˆ Test benchmark execution
      run: |
        source .venv/bin/activate
        uv run pytest src/core/tests/benchmark/config/test_settings_benchmark.py::TestSettingsBenchmark::test_settings_creation_benchmark --benchmark-only --benchmark-json=test-benchmark.json
    
    - name: ðŸ“„ Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: test-benchmark-results
        path: test-benchmark.json
        retention-days: 7
    
    - name: âœ… Benchmark configuration test passed
      run: echo "âœ… Benchmark configuration is working correctly"

  test-matrix:
    name: æ¸¬è©¦çŸ©é™£é…ç½® (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    if: github.event.inputs.test-component == 'matrix'
    
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.11", "3.12", "3.13"]
    
    steps:
    - name: ðŸ“¥ Checkout repository
      uses: actions/checkout@v4
    
    - name: ðŸ”§ Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
    
    - name: ðŸ Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: ðŸ”— Create virtual environment
      run: uv venv
    
    - name: ðŸ“¦ Install dependencies
      run: |
        source .venv/bin/activate
        uv sync --dev
    
    - name: ðŸ§ª Run simple test
      run: |
        source .venv/bin/activate
        uv run pytest src/core/tests/unit/config/test_base_settings.py::TestBaseCoreSettings::test_default_values -v
    
    - name: âœ… Matrix test passed
      run: echo "âœ… Python ${{ matrix.python-version }} configuration is working"

  summary:
    name: ðŸ“‹ æ¸¬è©¦æ‘˜è¦
    runs-on: ubuntu-latest
    needs: [test-basic, test-security, test-benchmark, test-matrix]
    if: always()
    
    steps:
    - name: ðŸ“Š Generate test summary
      run: |
        echo "## ðŸ§ª CI Configuration Test çµæžœæ‘˜è¦" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### æ¸¬è©¦çµæžœ" >> $GITHUB_STEP_SUMMARY
        echo "- **åŸºæœ¬é…ç½®**: ${{ needs.test-basic.result || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **å®‰å…¨æŽƒæ**: ${{ needs.test-security.result || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Benchmark**: ${{ needs.test-benchmark.result || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **çŸ©é™£æ¸¬è©¦**: ${{ needs.test-matrix.result || 'skipped' }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.test-basic.result }}" == "success" ] || [ "${{ needs.test-security.result }}" == "success" ] || [ "${{ needs.test-benchmark.result }}" == "success" ] || [ "${{ needs.test-matrix.result }}" == "success" ]; then
          echo "âœ… **CI é…ç½®æ¸¬è©¦é€šéŽï¼**" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **CI é…ç½®æ¸¬è©¦å¤±æ•—**" >> $GITHUB_STEP_SUMMARY
        fi