# ABOUTME: 主要的 CI/CD pipeline，負責程式碼品質檢查、多版本測試、效能測試和安全掃描
# ABOUTME: 支援 Python 3.11-3.13，並行執行不同類型的測試，自動更新 benchmark baseline

name: CI

on:
  push:
    branches: [ develop ]
  pull_request:
    branches: [ develop ]
  workflow_dispatch:
    inputs:
      python-version:
        description: 'Python version to test (leave empty for all versions)'
        required: false
        default: ''
        type: choice
        options:
          - ''
          - '3.11'
          - '3.12'
          - '3.13'
      test-type:
        description: 'Type of tests to run'
        required: false
        default: 'all'
        type: choice
        options:
          - 'all'
          - 'unit'
          - 'integration'
          - 'contract'
          - 'e2e'
          - 'benchmark'
          - 'quality-only'
      update-baseline:
        description: 'Update benchmark baseline after successful run'
        required: false
        default: false
        type: boolean
      skip-security:
        description: 'Skip security scanning'
        required: false
        default: false
        type: boolean

jobs:
  quality-check:
    name: 程式碼品質檢查
    runs-on: ubuntu-latest
    if: github.event.inputs.test-type != 'benchmark'
    
    steps:
    - name: 📥 Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: 🔧 Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
    
    - name: 🐍 Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"
    
    - name: 🔗 Create virtual environment
      run: uv venv
    
    - name: 📦 Install dependencies
      run: |
        source .venv/bin/activate
        uv sync --dev
    
    - name: 🎨 Check code formatting
      run: |
        source .venv/bin/activate
        uv run poe check-format
    
    - name: 🔍 Run linting
      run: |
        source .venv/bin/activate
        uv run poe lint
    
    - name: 🏷️ Run type checking
      run: |
        source .venv/bin/activate
        uv run poe check-types
    
    - name: 📊 Check dependencies
      run: |
        source .venv/bin/activate
        uv run poe check-deps
    
    - name: 🔐 Security scanning
      if: github.event.inputs.skip-security != 'true'
      run: |
        source .venv/bin/activate
        # Run security checks
        uv run safety check --json --output safety-report.json || true
        uv run bandit -r src/ -f json -o bandit-report.json || true
        uv run pip-audit --format=json --output=pip-audit-report.json || true
    
    - name: 📄 Upload security reports
      if: github.event.inputs.skip-security != 'true'
      uses: actions/upload-artifact@v4
      with:
        name: security-reports
        path: |
          safety-report.json
          bandit-report.json
          pip-audit-report.json
        retention-days: 30

  test-matrix:
    name: 測試 (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    needs: quality-check
    if: github.event.inputs.test-type != 'quality-only' && github.event.inputs.test-type != 'benchmark'
    
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.11', '3.12', '3.13']
    
    steps:
    - name: 📥 Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: 🔧 Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
    
    - name: 🐍 Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: 🔗 Create virtual environment
      run: uv venv
    
    - name: 📦 Install dependencies
      run: |
        source .venv/bin/activate
        uv sync --dev
    
    - name: 🧪 Run unit tests
      run: |
        source .venv/bin/activate
        uv run pytest -m unit --cov=src --cov-report=xml --cov-report=term-missing --verbose || true
    
    - name: 🧪 Run integration tests
      run: |
        source .venv/bin/activate
        uv run pytest -m integration --cov=src --cov-append --cov-report=xml --cov-report=term-missing --verbose || true
    
    - name: 🧪 Run contract tests
      run: |
        source .venv/bin/activate
        uv run pytest -m contract --cov=src --cov-append --cov-report=xml --cov-report=term-missing --verbose || true
    
    - name: 🧪 Run e2e tests
      run: |
        source .venv/bin/activate
        uv run pytest -m e2e --cov=src --cov-append --cov-report=xml --cov-report=term-missing --verbose || true
    
    - name: 📊 Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      with:
        files: ./coverage.xml
        flags: all-tests
        name: codecov-${{ matrix.python-version }}
        fail_ci_if_error: false
        token: ${{ secrets.CODECOV_TOKEN }}

  benchmark:
    name: 效能測試
    runs-on: ubuntu-latest
    needs: quality-check
    if: github.event.inputs.test-type == 'benchmark' || github.event.inputs.test-type == 'all' || github.event.inputs.test-type == ''
    
    steps:
    - name: 📥 Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: 🔧 Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
    
    - name: 🐍 Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"
    
    - name: 🔗 Create virtual environment
      run: uv venv
    
    - name: 📦 Install dependencies
      run: |
        source .venv/bin/activate
        uv sync --dev
    
    - name: 📈 Run benchmark tests
      run: |
        source .venv/bin/activate
        mkdir -p benchmarks/baselines benchmarks/results
        # Find existing baseline file (could be in platform-specific subdirectory)
        BASELINE_FILE=""
        if [ -f "benchmarks/baselines/benchmark-baseline.json" ]; then
          BASELINE_FILE="benchmarks/baselines/benchmark-baseline.json"
        elif ls benchmarks/baselines/*/????_baseline.json 1> /dev/null 2>&1; then
          BASELINE_FILE=$(ls -t benchmarks/baselines/*/????_baseline.json | head -1)
        fi
        
        if [ -n "$BASELINE_FILE" ]; then
          echo "📊 Running benchmark with baseline comparison..."
          echo "Using baseline: $BASELINE_FILE"
          uv run pytest src/core/tests/benchmark/ --benchmark-only --benchmark-compare="$BASELINE_FILE" --benchmark-json=benchmarks/results/benchmark-results.json
        else
          echo "📝 Creating initial baseline..."
          uv run pytest src/core/tests/benchmark/ --benchmark-only --benchmark-save=baseline --benchmark-storage=file://benchmarks/baselines --benchmark-json=benchmarks/results/benchmark-results.json
          # Copy to standard name for future comparisons
          if ls benchmarks/baselines/*/????_baseline.json 1> /dev/null 2>&1; then
            LATEST_FILE=$(ls -t benchmarks/baselines/*/????_baseline.json | head -1)
            cp "$LATEST_FILE" benchmarks/baselines/benchmark-baseline.json
            echo "✅ Initial baseline created and copied to standard name"
          fi
        fi
    
    - name: 📊 Copy benchmark results
      run: |
        # Copy benchmark results for artifact upload
        if [ -f "benchmarks/results/benchmark-results.json" ]; then
          cp benchmarks/results/benchmark-results.json benchmark-results.json
        fi
    
    - name: 💾 Update benchmark baseline
      if: github.ref == 'refs/heads/develop' && (github.event.inputs.update-baseline == 'true' || github.event_name == 'push')
      run: |
        source .venv/bin/activate
        mkdir -p benchmarks/baselines
        
        echo "🔄 Creating new benchmark baseline..."
        export BENCHMARK_STORAGE_PATH="benchmarks/baselines"
        uv run pytest src/core/tests/benchmark/ --benchmark-only --benchmark-save=baseline --benchmark-storage=file://benchmarks/baselines
        
        # Check if the benchmark file was created (in platform-specific subdirectory)
        if ls benchmarks/baselines/*/????_baseline.json 1> /dev/null 2>&1; then
          echo "✅ Baseline file created successfully"
          # Copy to standard name for consistency
          LATEST_FILE=$(ls -t benchmarks/baselines/*/????_baseline.json | head -1)
          cp "$LATEST_FILE" benchmarks/baselines/benchmark-baseline.json
          echo "✅ Baseline file copied to standard name: $LATEST_FILE -> benchmarks/baselines/benchmark-baseline.json"
        else
          echo "❌ No baseline file found"
          exit 1
        fi
        
        # Check if baseline actually changed
        if git diff --quiet benchmarks/baselines/; then
          echo "ℹ️  No changes in baseline - skipping commit"
          exit 0
        fi
        
        # Commit baseline if it changed
        echo "📝 Committing baseline changes..."
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add benchmarks/baselines/
        git commit -m "ci(workspace): 更新 benchmark baseline [$(date -u +\"%Y-%m-%d %H:%M:%S UTC\")]"
        
        # Try to push with error handling
        echo "🚀 Pushing baseline changes..."
        if git push; then
          echo "✅ Baseline successfully pushed"
        else
          echo "❌ Failed to push baseline - check repository permissions"
          echo "💡 You may need to enable 'Read and write permissions' in repository Settings > Actions > General"
          exit 1
        fi
    
    - name: 📄 Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: |
          benchmark-results.json
          benchmarks/
        retention-days: 30

  notify-failure:
    name: 失敗通知
    runs-on: ubuntu-latest
    needs: [quality-check, test-matrix, benchmark]
    if: failure()
    
    steps:
    - name: 📧 Send failure notification
      if: env.SLACK_WEBHOOK_URL != ''
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        channel: '#ci-alerts'
        text: |
          ❌ CI Pipeline 失敗！
          
          **Repository**: ${{ github.repository }}
          **Branch**: ${{ github.ref }}
          **Commit**: ${{ github.sha }}
          **Author**: ${{ github.actor }}
          **Trigger**: ${{ github.event_name }}
          
          請檢查 GitHub Actions 查看詳細資訊。
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  summary:
    name: 📋 CI 摘要
    runs-on: ubuntu-latest
    needs: [quality-check, test-matrix, benchmark]
    if: always()
    
    steps:
    - name: 📊 Generate summary
      run: |
        echo "## 🚀 CI Pipeline 執行摘要" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 執行配置" >> $GITHUB_STEP_SUMMARY
        echo "- **Python 版本**: ${{ matrix.python-version || 'All (3.11-3.14)' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **測試類型**: ${{ github.event.inputs.test-type || 'all' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **觸發事件**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **分支**: ${{ github.ref }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 執行結果" >> $GITHUB_STEP_SUMMARY
        echo "- **程式碼品質**: ${{ needs.quality-check.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **測試矩陣**: ${{ needs.test-matrix.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **效能測試**: ${{ needs.benchmark.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.quality-check.result }}" == "success" ] && [ "${{ needs.test-matrix.result }}" == "success" ] && [ "${{ needs.benchmark.result }}" == "success" ]; then
          echo "✅ **所有檢查都通過！**" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **部分檢查失敗，請檢查上述結果**" >> $GITHUB_STEP_SUMMARY
        fi